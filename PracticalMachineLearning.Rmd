---
title: "Practical Machine Learning-Course Prject"
author: "ASpilotros"
date: "Thursday, November 17, 2016"
output: html_document
---

#Assignment
Data used in this project are taken from:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

The data sets report data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.
The goal of this project is to "predict the manner in which they did the exercise" i.e. predict the outcome of the variable "classe" in the training and testing data sets.
Further, Professor Leek states that this report should describe:

* "how you built your model"
* "how you used cross validation"
* "what you think the expected out of sample error is"
* "why you made the choices you did"

Ultimately, the prediction model is to be run on the test data to predict the outcome of 20 different test cases.

#Results

```{r, warning=FALSE,message=FALSE}
#Loading Libraries
library(AppliedPredictiveModeling)
library(caret)
library(rattle)
library(rpart.plot)
library(randomForest)
```

```{r, warning=FALSE,message=FALSE}
#Loading Data
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
#head(training,10)
```
I decided to strip out the columns with too many missing values (NA>=50%) and the first 7 columns which are not relevant to assess the quality of the activity.

```{r, warning=FALSE,message=FALSE}
#Cleaning Data

dims <- dim(training)
na <- sapply(training,function(x)sum(is.na(x)))
trainingF <- training[,names(which(na<0.50*dims[1]))]
trainingF<-trainingF[,-c(1:7)]
trainingFF<-trainingF[,-53]
```
I check if some variables have low variance
```{r}
zeroVariance=nearZeroVar(trainingFF, saveMetrics = TRUE) #this gives you a list. Column 4 is a logic vector
sum(zeroVariance[,4]) # to check if there are near zero variance variables
```
None of the columns shows a near 0 variance. I perform the same transformation on the testing dataset
```{r}
testingFF<-testing[colnames(trainingFF)]

```
##Random Forest
A test set is already provided for this project, however, in view of the large amount of observations, we can afford to split the training dataset in a training and a test set. I will use the test set previously provided as validation.
The classification project proposed could be addressed with different techniques. Looking at the number of the remaining predictors (52), our preference goes to methods that explore carefully a limited predictor subspace. Random forest is suited for this purpose and I decided to run the algorithm with different number of predictors per run (from 1 to 52 which correspond to simple bagging). Note that cross validation is "included" in random forest algorithm:recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations.  
```{r}
set.seed(101)
train=sample(1:19622,13736)
##Random Forest model
oob.err=double(52)
test.err=double(52)
for(mtry in 1:52){
  
  fit=randomForest(classe~.,data=trainingF,subset=train, mtry=mtry,ntree=200)
  oob.err[mtry]=(sum(fit$confusion)-sum(diag(fit$confusion)))/sum(fit$confusion) #the elements on the diagonal of the confusion matrix are the correct predicitons, the ones off diagonal are the wrong predictions
  pred=predict(fit, trainingF[-train,])
  predmatrix=table(pred,trainingF$classe[-train])
  test.err[mtry]=(sum(predmatrix)-sum(diag(predmatrix)))/sum(predmatrix)
  cat(mtry," ") #to print the for cycle index
}
#Let's have a look to the results
matplot(1:52,cbind(test.err,oob.err),pch=19,col=c("red","blue"),type = "b", ylab = "Error", xlab='N predictors')
legend("topright",legend=c("OOB","Test"),pch=19,col=c("red","blue"))
```

The plot produced indicates that 9 is the best choice for the parameter mtry in randomForest algorithm.For bagged/RF classification trees, we add up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all B trees. This is a way to order the predictors by importance.

```{r}
bestmodel=randomForest(classe~.,data=trainingF,subset=train, mtry=9,ntree=200)
bestmodel
varImpPlot(bestmodel,cex=0.7,pch=19)
```

##Expected test error

The expected test error is represented by the out of bag error OOB and it is about 0.52%

##Predictions and writing results

Now let's predict on the testing dataset and generate the files to submit

```{r}
predictions_bestmodel <- predict(bestmodel, testing, type = "class")
write_file = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

write_file(predictions_bestmodel)
```




